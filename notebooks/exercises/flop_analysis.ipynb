{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/pymc-devs/pytensor-workshop/blob/main/notebooks/exercises/flop_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfc5gS281Z9R"
   },
   "source": [
    "**ðŸ’¡ To better engage gray mass we suggest you turn off Colab AI autocompletion in `Tools > Settings > AI Assistance`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6kRJ_NXIsk5m"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install 'pytensor>=2.28.3'\n",
    "try:\n",
    "    import pytensor_workshop\n",
    "except ModuleNotFoundError:\n",
    "    !pip install git+https://github.com/pymc-devs/pytensor-workshop.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2JtifJXCsspU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph import Apply, Variable, rewrite_graph\n",
    "from pytensor.graph.fg import FunctionGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HUB7tr_VsrCe"
   },
   "outputs": [],
   "source": [
    "from pytensor_workshop import test, FullHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQIMFJTqBEB0"
   },
   "source": [
    "## Estimating the number of flop in a PyTensor graph\n",
    "\n",
    "This Notebook challenges you to define a graph transformation, by introspecting a limited subset of graphs and defining symbolic functions based on what it's there.\n",
    "\n",
    "The theme is to define a graph -> flop [floating point operations]() count estimation transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CXoWubJCu2n"
   },
   "source": [
    "## Exercise 1: Flop in a matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaXLUdjNDEmz"
   },
   "source": [
    "Create a function that takes as input a variable that is itself the output of a PyTensor dot, and returns the number of flop of that operation by analyzing the inputs static shapes (retrieved via variable.type.shpae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5TzApQEEL42",
    "outputId": "b0657833-5ec7-4464-d3d1-dd95f5677020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot [id A] shape=(512, 256)\n",
      " â”œâ”€ x [id B] shape=(512, 64)\n",
      " â””â”€ y [id C] shape=(64, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f1decd33130>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pt.matrix(\"x\", shape=(512, 64))\n",
    "y = pt.matrix(\"y\", shape=(64, 256))\n",
    "\n",
    "outs = pt.dot(x, y)\n",
    "outs.dprint(print_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxEyPNLMBIs0",
    "outputId": "84564b0a-93cd-4921-fa26-f86c9eeb8b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot [id A] shape=(512, 256)\n",
      " â”œâ”€ x [id B] shape=(512, 64)\n",
      " â””â”€ y [id C] shape=(64, 256)\n"
     ]
    }
   ],
   "source": [
    "x = pt.matrix(\"x\", shape=(512, 64))\n",
    "y = pt.matrix(\"y\", shape=(64, 256))\n",
    "\n",
    "outs = pt.dot(x, y)\n",
    "outs.dprint(print_shape=True)\n",
    "\n",
    "def dot_flop_fn(var: pt.TensorVariable) -> int:\n",
    "    ...\n",
    "\n",
    "@test\n",
    "def test_static_number_of_dot_flop(flop_fn):\n",
    "    rng = np.random.default_rng()\n",
    "    m, p, n = [int(i) for i in rng.integers(512, size=(3,))]\n",
    "    \n",
    "    x = pt.matrix(\"x\", shape=(m, p))\n",
    "    y = pt.matrix(\"y\", shape=(p, n))\n",
    "    out = pt.dot(x, y)\n",
    "\n",
    "    flop_res = flop_fn(out) \n",
    "    np.testing.assert_allclose(flop_res , m * n * (2 * p - 1))\n",
    "\n",
    "# test_static_number_of_dot_flop(dot_flop_fn)  # uncomment me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq92gKgmEmUO"
   },
   "source": [
    "## Exercise 2: Flop based on symbolic shape\n",
    "\n",
    "Static shapes are nice but not always available. In this exercise we would like to tweak the function to work on the shapes of PyTensor variables. Given the output of a dot, can you create an expression that computes the number of flop as a function of the inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ymHJmuq9Bckq"
   },
   "outputs": [],
   "source": [
    "def dot_flop_fn(var: pt.TensorVariable) -> pt.TensorVariable:\n",
    "    ...\n",
    "\n",
    "@test\n",
    "def test_symbolic_number_of_dot_flop(flop_fn):\n",
    "    rng = np.random.default_rng()\n",
    "    m, p, n = [int(i) for i in rng.integers(512, size=(3,))]\n",
    "\n",
    "    x = pt.matrix(\"x\", shape=(None, p))\n",
    "    y = pt.matrix(\"y\", shape=(p, None))\n",
    "    out = pt.dot(x, y)\n",
    "\n",
    "    out_flop = flop_fn(out)\n",
    "\n",
    "    x_test = rng.normal(size=(m, p))\n",
    "    y_test = rng.normal(size=(p, n))\n",
    "\n",
    "    res = out_flop.eval({x: x_test, y: y_test})\n",
    "    np.testing.assert_allclose(res, m * n * (2 * p - 1))\n",
    "\n",
    "# test_symbolic_number_of_dot_flop(dot_flop_fn)  # uncomment me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbqDHJcnFsPk"
   },
   "source": [
    "## Exercise 3: Flop of an Elemwise Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aBkbqqvF1Mo"
   },
   "source": [
    "Let's pretend all Elemwise functions have a constant flop cost ([not true](https://latkin.org/blog/2014/11/09/a-simple-benchmark-of-various-math-operations/)). Write a function that computes the number of flop of an Elemwise expression. Your function should work both with univariate (like exp / cos) and bivariate functions (like add / mul).\n",
    "\n",
    "From now on we always want you to return a symbolic expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Tg6oEABPCuq",
    "outputId": "c4d00dbe-0418-46da-cea0-3ccbc6f78939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add [id A] shape=(3, 2, ?)\n",
      " â”œâ”€ x [id B] shape=(1, 2, ?)\n",
      " â””â”€ y [id C] shape=(3, 1, ?)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f1decd33130>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pt.tensor(\"x\", shape=(1, 2, None))\n",
    "y = pt.tensor(\"y\", shape=(3, 1, None))\n",
    "out = pt.add(x, y)\n",
    "out.dprint(print_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5RcNVqf0P7_n"
   },
   "outputs": [],
   "source": [
    "def elemwise_flop_fn(var: pt.TensorVariable) -> pt.TensorVariable:\n",
    "    ...\n",
    "\n",
    "@test\n",
    "def test_univariate_elemwise_flop(flop_fn):\n",
    "    rng = np.random.default_rng(1)\n",
    "    a, b, = [int(i) for i in rng.integers(512, size=(2,))]\n",
    "    \n",
    "    x = pt.tensor(\"x\", shape=(None, None))\n",
    "\n",
    "    out = pt.exp(x)\n",
    "    out_flop = flop_fn(out)\n",
    "\n",
    "    x_test = rng.normal(size=(a, b))\n",
    "    res = out_flop.eval({x: x_test})\n",
    "    np.testing.assert_allclose(res, a * b)\n",
    "\n",
    "# test_univariate_elemwise_flop(elemwise_flop_fn)  # uncomment me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y7nPY2lSKbl"
   },
   "source": [
    "Remember that Elemwise operations with multiple inputs implicitly broadcast the inputs. You'll have to take that into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dUZ5FMN0QsJg"
   },
   "outputs": [],
   "source": [
    "@test\n",
    "def test_bivariate_elemwise_flop(flop_fn):\n",
    "    rng = np.random.default_rng()\n",
    "    a, b, c = [int(i) for i in rng.integers(512, size=(3,))]\n",
    "    \n",
    "    x = pt.tensor(\"x\", shape=(a, 1, None))\n",
    "    y = pt.tensor(\"y\", shape=(1, b, None))\n",
    "\n",
    "    out = x * y\n",
    "\n",
    "    out_flop = flop_fn(out)\n",
    "\n",
    "    res = out_flop.eval({\n",
    "        x: np.random.normal(size=(a, 1, c)),\n",
    "        y: np.random.normal(size=(1, b, c)),\n",
    "    })\n",
    "    np.testing.assert_allclose(res, a * b * c)\n",
    "\n",
    "# test_bivariate_elemwise_flop(elemwise_flop_fn)  # uncomment me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx_-gtJwScnp"
   },
   "source": [
    "## Exercise 4: Flop of a computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orYgdWgPSjjY"
   },
   "source": [
    "Now that we got a hang for how to work with single nodes, it's time to take a look at larger computational graphs. Extend your function to work both with Elemwise and Dot nodes, whose inputs may also be the output of further computations. We want a final expression that estimates the number of float point operations in the **whole graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfxHJtpMRDwn",
    "outputId": "0037a80c-731b-49b0-c822-2a0795469c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot [id A] shape=(512, 256)\n",
      " â”œâ”€ Add [id B] shape=(512, 64)\n",
      " â”‚  â”œâ”€ x [id C] shape=(512, 64)\n",
      " â”‚  â””â”€ x [id C] shape=(512, 64)\n",
      " â””â”€ Exp [id D] shape=(64, 256)\n",
      "    â””â”€ y [id E] shape=(64, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f1decd33130>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pt.matrix(\"x\", shape=(512, 64))\n",
    "y = pt.matrix(\"y\", shape=(64, 256))\n",
    "out = pt.dot(x + x, pt.exp(y))\n",
    "out.dprint(print_shape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBohFMpwTF6N"
   },
   "source": [
    "Do you still remember how to walk up a Pytensor graph? If not here is a quick reminder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcMpEnrKTE-8",
    "outputId": "dec4a85f-8c50-4841-b903-ecd8bda0a795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot.0 <class 'pytensor.tensor.variable.TensorVariable'>\n",
      "dot(Add.0, Exp.0) <class 'pytensor.graph.basic.Apply'>\n",
      "[Add.0, Exp.0] <class 'list'>\n",
      "Add.0 <class 'pytensor.tensor.variable.TensorVariable'>\n",
      "Add(x, x) <class 'pytensor.graph.basic.Apply'>\n"
     ]
    }
   ],
   "source": [
    "print(out, type(out))\n",
    "print(out.owner, type(out.owner))\n",
    "print(out.owner.inputs, type(out.owner.inputs))\n",
    "print(out.owner.inputs[0], type(out.owner.inputs[0]))\n",
    "print(out.owner.inputs[0].owner, type(out.owner.inputs[0].owner))\n",
    "# ... and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI6WdodzTh3v"
   },
   "source": [
    "Let's start with an easy test, just nested univariate elemwise functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H3NT773BTBmu"
   },
   "outputs": [],
   "source": [
    "def graph_flop_fn(var: pt.TensorVariable) -> pt.TensorVariable:\n",
    "    ...\n",
    "    \n",
    "\n",
    "@test\n",
    "def test_nested_univariate_elemwise_flop(flop_fn):\n",
    "    rng = np.random.default_rng()\n",
    "    a, b, = [int(i) for i in rng.integers(512, size=(2,))]\n",
    "    \n",
    "    x = pt.tensor(\"x\", shape=(None, None))\n",
    "    x_test = rng.normal(size=(a, b))\n",
    "\n",
    "    for i in range(4):\n",
    "        print(f\"Testing nesting depth: {i}\")\n",
    "        out = x\n",
    "        for j in range(i):\n",
    "            out = pt.exp(out)\n",
    "        flop_res = flop_fn(out).eval({x: x_test}, on_unused_input=\"ignore\")\n",
    "        np.testing.assert_allclose(flop_res, (a * b) * i)\n",
    "\n",
    "# test_nested_univariate_elemwise_flop(graph_flop_fn)  # uncomment me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TA7pcTZOXyq"
   },
   "source": [
    "Now let's mix everything together. One thing you need to be careful is that a variable may show up multiple times in a complex graph, and we shouldn't double count the number of flop. This is highlighted in the two similar but subtly different graphs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmArXNYU9MLk",
    "outputId": "d3af3b40-bb59-45a7-d640-e1578a40a2a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add [id A]\n",
      " â”œâ”€ dot [id B]\n",
      " â”‚  â”œâ”€ x [id C]\n",
      " â”‚  â””â”€ y [id D]\n",
      " â””â”€ Exp [id E]\n",
      "    â””â”€ dot [id B]\n",
      "       â””â”€ Â·Â·Â·\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f1decd33130>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pt.matrix(\"x\", shape=(512, 64))\n",
    "y = pt.matrix(\"y\", shape=(64, 256))\n",
    "\n",
    "xy_dot = pt.dot(x, y)\n",
    "out = xy_dot + pt.exp(xy_dot)\n",
    "out.dprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zP0lQeh-9WQ4",
    "outputId": "fa29a02b-18ca-4fa9-81ef-b303153e9b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add [id A]\n",
      " â”œâ”€ dot [id B]\n",
      " â”‚  â”œâ”€ x [id C]\n",
      " â”‚  â””â”€ y [id D]\n",
      " â””â”€ Exp [id E]\n",
      "    â””â”€ dot [id F]\n",
      "       â”œâ”€ x [id C]\n",
      "       â””â”€ y [id D]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f1decd33130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pt.matrix(\"x\", shape=(512, 64))\n",
    "y = pt.matrix(\"y\", shape=(64, 256))\n",
    "\n",
    "xy_dot1 = pt.dot(x, y)\n",
    "xy_dot2 = pt.dot(x, y)\n",
    "\n",
    "out = xy_dot1 + pt.exp(xy_dot2)\n",
    "out.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG0UBsta9eJm"
   },
   "source": [
    "The first graph reuses dotxy twice (hence the ellipsis in the dprint), while the second (as of now) computes it twice. Your function should realize this difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MzXuCBc-9dNY"
   },
   "outputs": [],
   "source": [
    "from pytensor.tensor.elemwise import Elemwise\n",
    "\n",
    "def graph_flop_fn(var: pt.TensorVariable) -> pt.TensorVariable:\n",
    "    ...\n",
    "\n",
    "@test\n",
    "def test_elemwise_and_dot_flop(flop_fn):\n",
    "    rng = np.random.default_rng()\n",
    "    m, p, n, = [int(i) for i in rng.integers(512, size=(3,))]\n",
    "    x = pt.matrix(\"x\", shape=(m, None))\n",
    "    y = pt.matrix(\"y\", shape=(None, n))\n",
    "\n",
    "    x_test = rng.normal(size=(m, p))\n",
    "    y_test = rng.normal(size=(p, n))\n",
    "\n",
    "    # Case 1 reused dot\n",
    "    print(\"Testing case 1: reused dot\")\n",
    "    xy_dot = pt.dot(x, y)\n",
    "    out = xy_dot + pt.exp(xy_dot)\n",
    "    flop_res = flop_fn(out).eval({x: x_test, y: y_test})\n",
    "    np.testing.assert_allclose(flop_res, m * n * (2 * p - 1) + 2 * (m * n))\n",
    "\n",
    "    # Case 2 duplicate dot\n",
    "    print(\"Testing case 2: duplicate dot\")\n",
    "    xy_dot1 = pt.dot(x, y)\n",
    "    xy_dot2 = pt.dot(x, y)\n",
    "    out = xy_dot1 + pt.exp(xy_dot2)\n",
    "    flop_fn_res = flop_fn(out).eval({x: x_test, y: y_test})\n",
    "    np.testing.assert_allclose(flop_fn_res, 2 * (m * n * (2 * p - 1)) + 2 * (m * n))\n",
    "\n",
    "    # Case 3 elemwise on the inputs\n",
    "    print(\"Testing case 3: dot and elemwise\")\n",
    "    x_exp = pt.exp(x)\n",
    "    y_exp = pt.exp(y)\n",
    "    out = pt.dot(x_exp, y_exp)\n",
    "    flop_res = flop_fn(out).eval({x: x_test, y: y_test})\n",
    "    np.testing.assert_allclose(flop_res, m * n * (2 * p - 1) + m * p + p * n)\n",
    "\n",
    "# test_elemwise_and_dot_flop(graph_flop_fn)  # uncomment me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CdBTN8pOX1r"
   },
   "source": [
    "# Exercise 5: Operations that don't involve float point operations\n",
    "\n",
    "For our final demonstration we need our function to handle operations that don't involve float point operations. We will have indexing in the graph, which should only affect flop indirectly by reducing the shape of the operations used downstream.\n",
    "\n",
    "Expand the graph_flop_fn, to return the right result for a graph of this sort:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSpJacmdF0S0",
    "outputId": "ae4957f6-dfdc-45d6-b73b-20492299530e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos [id A] shape=(1, 64)\n",
      " â””â”€ Subtensor{start:stop} [id B] shape=(1, 64)\n",
      "    â”œâ”€ Exp [id C] shape=(512, 64)\n",
      "    â”‚  â””â”€ x [id D] shape=(512, 64)\n",
      "    â”œâ”€ 0 [id E] shape=()\n",
      "    â””â”€ 1 [id F] shape=()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f1decd33130>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pt.matrix(\"x\", shape=(512, 64))\n",
    "y = pt.exp(x)\n",
    "z = y[0:1]\n",
    "out = pt.cos(z)\n",
    "\n",
    "out.dprint(print_shape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OijW0hVS-7v3"
   },
   "source": [
    "As always it may be useful to check what kind of operation we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCOpsqR9-0R6",
    "outputId": "32d67c33-be77-451d-c59e-18fa691b8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Subtensor(idx_list=(slice(ScalarType(int64), ScalarType(int64), None),)),\n",
       " pytensor.tensor.subtensor.Subtensor)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(z.owner.op, type(z.owner.op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LWpjTUwazNx8"
   },
   "outputs": [],
   "source": [
    "def graph_flop_fn(var: pt.TensorVariable) -> pt.TensorVariable:\n",
    "    ...\n",
    "\n",
    "@test\n",
    "def test_indexing_flop(flop_fn):\n",
    "    rng = np.random.default_rng()\n",
    "    a, b, = [int(i) for i in rng.integers(512, size=(2,))]\n",
    "    x = pt.tensor(\"x\", shape=(None, None))\n",
    "    x_test = rng.normal(size=(a, b))\n",
    "\n",
    "    y = pt.exp(x)\n",
    "    z = y[0:1]\n",
    "    out = pt.cos(z)\n",
    "    flop_fn_res = flop_fn(out).eval({x: x_test})\n",
    "    np.testing.assert_allclose(flop_fn_res, a * b + b)\n",
    "\n",
    "# test_indexing_flop(graph_flop_fn)  # uncomment me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c__qTzl3_PuM"
   },
   "source": [
    "## Wrap-up: seeing it in action\n",
    "\n",
    "Let's use the new fancy graph_flop_fn, to see how much we benefit from different different graph rewrites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaulTcY4_OaX",
    "outputId": "0402748b-e0d3-4458-e8ff-b6968387886f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtensor{i, j} [id A] shape=()\n",
      " â”œâ”€ Add [id B] shape=(512, 256)\n",
      " â”‚  â”œâ”€ dot [id C] shape=(512, 256)\n",
      " â”‚  â”‚  â”œâ”€ Cos [id D] shape=(512, 64)\n",
      " â”‚  â”‚  â”‚  â””â”€ x [id E] shape=(512, 64)\n",
      " â”‚  â”‚  â””â”€ Exp [id F] shape=(64, 256)\n",
      " â”‚  â”‚     â””â”€ y [id G] shape=(64, 256)\n",
      " â”‚  â””â”€ dot [id H] shape=(512, 256)\n",
      " â”‚     â”œâ”€ Cos [id D] shape=(512, 64)\n",
      " â”‚     â”‚  â””â”€ Â·Â·Â·\n",
      " â”‚     â””â”€ Exp [id F] shape=(64, 256)\n",
      " â”‚        â””â”€ Â·Â·Â·\n",
      " â”œâ”€ 0 [id I] shape=()\n",
      " â””â”€ -1 [id J] shape=()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f1decd33130>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pt.matrix(\"x\", shape=(512, 64))\n",
    "y = pt.matrix(\"y\", shape=(64, 256))\n",
    "\n",
    "x_cos = pt.cos(x)\n",
    "y_exp = pt.exp(y)\n",
    "dot1 = pt.dot(x_cos, y_exp)\n",
    "dot2 = pt.dot(x_cos, y_exp)\n",
    "out = (dot1 + dot2)[0, -1]\n",
    "out.dprint(print_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A79hfMD4BVYx",
    "outputId": "e55889c5-ba44-440b-9870-78702ff71604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add [id A] shape=()\n",
      " â”œâ”€ dot [id B] shape=()\n",
      " â”‚  â”œâ”€ Cos [id C] shape=(64,)\n",
      " â”‚  â”‚  â””â”€ Subtensor{i} [id D] shape=(64,)\n",
      " â”‚  â”‚     â”œâ”€ x [id E] shape=(512, 64)\n",
      " â”‚  â”‚     â””â”€ 0 [id F] shape=()\n",
      " â”‚  â””â”€ Exp [id G] shape=(64,)\n",
      " â”‚     â””â”€ Subtensor{:, i} [id H] shape=(64,)\n",
      " â”‚        â”œâ”€ y [id I] shape=(64, 256)\n",
      " â”‚        â””â”€ -1 [id J] shape=()\n",
      " â””â”€ dot [id B] shape=()\n",
      "    â””â”€ Â·Â·Â·\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f1decd33130>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_graph(out).dprint(print_shape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that dot inputs need not be 2D, you may need to tweak your `graph_flop_fn` to handle this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YGrDczbcBZ8e"
   },
   "outputs": [],
   "source": [
    "def graph_flop_fn(var: pt.TensorVariable) -> pt.TensorVariable:\n",
    "    ...\n",
    "\n",
    "@test\n",
    "def before_and_after(flop_fn):\n",
    "    x = pt.matrix(\"x\", shape=(512, 64))\n",
    "    y = pt.matrix(\"y\", shape=(64, 256))\n",
    "\n",
    "    x_test = np.random.normal(size=(512, 64))\n",
    "    y_test = np.random.normal(size=(64, 256))\n",
    "\n",
    "    x_cos = pt.cos(x)\n",
    "    y_exp = pt.exp(y)\n",
    "    dot1 = pt.dot(x_cos, y_exp)\n",
    "    dot2 = pt.dot(x_cos, y_exp)\n",
    "    out = (dot1 + dot2)[0, -1]\n",
    "    # out.dprint(print_shape=True)\n",
    "\n",
    "    before = flop_fn(out)\n",
    "    before_eval = before.eval({x: x_test, y: y_test})\n",
    "\n",
    "    out_rewritten = rewrite_graph(out)\n",
    "    after = flop_fn(out_rewritten)\n",
    "    after_eval = after.eval({x: x_test, y: y_test})\n",
    "\n",
    "    print(f\"flop before rewritting: {before_eval}\")\n",
    "    print(f\"flop after rewriting:   {after_eval}\")\n",
    "\n",
    "# before_and_after(graph_flop_fn)  # uncomment me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeOUN0ReCbVe"
   },
   "source": [
    "Let's zoom in and see the effect after each rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CInIy7DeCaYi",
    "outputId": "41769d6d-e0f1-4269-b374-00447a38d370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewriting: rewrite local_subtensor_lift replaces Subtensor{i, j}.0 of Subtensor{i, j}(Add.0, 0, -1) with Add.0 of Add(Subtensor{i, j}.0, Subtensor{i, j}.0)\n",
      "rewriting: rewrite local_subtensor_of_dot replaces Subtensor{i, j}.0 of Subtensor{i, j}(dot.0, 0, -1) with dot.0 of dot(Subtensor{i}.0, Subtensor{:, i}.0)\n",
      "rewriting: rewrite local_subtensor_lift replaces Subtensor{i}.0 of Subtensor{i}(Cos.0, 0) with Cos.0 of Cos(Subtensor{i}.0)\n",
      "rewriting: rewrite local_subtensor_lift replaces Subtensor{:, i}.0 of Subtensor{:, i}(Exp.0, -1) with Exp.0 of Exp(Subtensor{:, i}.0)\n"
     ]
    }
   ],
   "source": [
    "with pytensor.config.change_flags(optimizer_verbose=True):\n",
    "    out_rewritten = rewrite_graph(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gs3rooI1CsXz",
    "outputId": "7cdde41d-9c97-4247-8a13-7732852fe3d8"
   },
   "outputs": [],
   "source": [
    "@test\n",
    "def flop_step_by_step(flop_fn):\n",
    "    x = pt.matrix(\"x\", shape=(512, 64))\n",
    "    y = pt.matrix(\"y\", shape=(64, 256))\n",
    "    rng = np.random.default_rng()\n",
    "    x_test = rng.normal(size=x.type.shape)\n",
    "    y_test = rng.normal(size=y.type.shape)\n",
    "    \n",
    "    x_cos = pt.cos(x)\n",
    "    y_exp = pt.exp(y)\n",
    "    dot1 = pt.dot(x_cos, y_exp)\n",
    "    dot2 = pt.dot(x_cos, y_exp)\n",
    "    out = (dot1 + dot2)[0, -1]\n",
    "    \n",
    "    fg = FunctionGraph(outputs=[out], copy_inputs=False)\n",
    "    history = FullHistory()\n",
    "    fg.attach_feature(history)\n",
    "    \n",
    "    rewrite_graph(fg)\n",
    "    \n",
    "    # Replay rewrites\n",
    "    history.start()\n",
    "    pytensor.dprint(fg, print_shape=True)\n",
    "    flop_res = flop_fn(fg.outputs[0]).eval({x: x_test, y: y_test}, on_unused_input=\"ignore\")\n",
    "    print(f\"Flop estimate: {flop_res:,}\")\n",
    "\n",
    "    for i in range(6):\n",
    "        print()\n",
    "        print(\">>> \", end=\"\")\n",
    "\n",
    "        with pytensor.config.change_flags(optimizer_verbose=True):\n",
    "            fg_checkpoint = history.next()            \n",
    "        fg_checkpoint.dprint(print_shape=True)\n",
    "\n",
    "        flop_res = flop_fn(fg.outputs[0]).eval({x: x_test, y: y_test}, on_unused_input=\"ignore\")\n",
    "        print(f\"Flop estimate: {flop_res:,}\")\n",
    "\n",
    "# flop_step_by_step(graph_flop_fn)  # Uncomment me"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytensor-dev",
   "language": "python",
   "name": "pytensor-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
