{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e65ca97",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Rewrites\n",
    "\n",
    "We have spoken ad nauseum about the usefulness of rewrites in pytensor. In this notebook, we will see how they are actually implemented. \n",
    "\n",
    "Out motivating example will be to reproduce some of the functionality of the [COmpositional Linear Algebra](https://github.com/wilson-labs/cola) (COLA) library. COLA is a widely-used package that introduces linear algebra primitives into jax, allowing it to save a lot of computation by being smarter about how to do expensive linear algebra operations.\n",
    "\n",
    "This should sound exactly like pytensor rewrites, because it is! We implement much of the same functionality. In this notebook, we will see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0af5d",
   "metadata": {},
   "source": [
    "# Speeding up inversion\n",
    "\n",
    "Matrix inversion is an $\\mathcal{O}(n^3)$ operation, which obviously sucks. But there are conditions in which it can be much faster.\n",
    "\n",
    "For example, if we know ahead of time the matrix is square and *diagonal*, then the inverse of the matrix is just the recipricol of the elements on the main diagonal. Computing this way is much, much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5113f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "SEED = sum(map(ord, 'Linag Rewrites'))\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "x_diag = np.diag(rng.normal(size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d62226",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pt.dmatrix('x')\n",
    "x_inv = pt.linalg.inv(x)\n",
    "f = pytensor.function([x], x_inv)\n",
    "f2 = pytensor.function([x], pt.diag(1 / pt.diag(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57516d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixInverse [id A] 0\n",
      " └─ x [id B]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.dprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae48d214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show that these are the same\n",
    "np.allclose(f(x_diag), f2(x_diag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f22c9",
   "metadata": {},
   "source": [
    "With a $1000 \\times 1000$ matrix we're already 87 times faster. The savings will increase as the matrix size grows, because of the non-linear time complexity of matrix inversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91150470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.1 ms ± 1.11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "277 μs ± 6.88 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "Dumb invert is 78 times slower than smart\n"
     ]
    }
   ],
   "source": [
    "dumb_invert_time = %timeit -o f(x_diag)\n",
    "smart_invert_time = %timeit -o f2(x_diag)\n",
    "print(f'Dumb invert is {dumb_invert_time.best / smart_invert_time.best:0.0f} times slower than smart')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7a5b9",
   "metadata": {},
   "source": [
    "## Side node: We already do this :)\n",
    "\n",
    "But there's some caveats, as we will see. Mainly, we have to do it in a way that gives pytensor a hint that we're inverting a diagonal matrix. If we just invert an anonymous matrix, it won't be able to reason about it and will be forced to do the most general thing (that's why the `f` function is so slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "427f94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inv2 = pt.linalg.inv(pt.eye(x.shape[0]) * pt.diag(x))\n",
    "f3 = pytensor.function([x], x_inv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eeeeb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truediv [id A] 4\n",
      " ├─ Eye{dtype='float64'} [id B] 3\n",
      " │  ├─ Shape_i{0} [id C] 2\n",
      " │  │  └─ x [id D]\n",
      " │  ├─ Shape_i{0} [id C] 2\n",
      " │  │  └─ ···\n",
      " │  └─ 0 [id E]\n",
      " └─ ExpandDims{axis=0} [id F] 1\n",
      "    └─ ExtractDiag{offset=0, axis1=0, axis2=1, view=False} [id G] 0\n",
      "       └─ x [id D]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3.dprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d88a9065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 μs ± 16.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "pt_invert_time = %timeit -o f3(x_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee2c1d",
   "metadata": {},
   "source": [
    "# Re-implementing this rewrite\n",
    "\n",
    "Rewrites a just functions, so they're very easy to implement! They all look like this:\n",
    "\n",
    "```\n",
    "@node_rewriter([TargetOp])\n",
    "def my_rewrite(fgraph, node) -> list[new_outputs]\n",
    "```\n",
    "\n",
    "- `node_rewriter` is a function wrapper that takes our rewrite and converts it to (wait for it) a `NodeRewriter`. There are other kinds of rewriters too, but 99% of the time you'll want `NodeRewriter`. This is a rewrite that targets a single type of operation (the `TargetOp`) and swaps its outputs for the new outputs returned by the function.\n",
    "\n",
    "- `fgraph` is the full `FunctionGraph` associated with the graph being rewritten. Having this allows you to reason globally about the computation, if necessary\n",
    "\n",
    "- `node` is the actual `Apply` node associated with the `TargetOp`.\n",
    "\n",
    "Let's target `Inv`, and directly replace it with `diag(1 / diag(x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab69676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytensor.tensor.nlinalg import MatrixInverse\n",
    "from pytensor.graph.rewriting.basic import (\n",
    "    in2out,\n",
    "    node_rewriter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5424971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@node_rewriter([MatrixInverse])\n",
    "def inverse_to_diag_reciprocal_dumb(fgraph, node):\n",
    "    [x] = node.inputs\n",
    "    return [pt.diag(1 / pt.diag(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d301235f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pytensor.graph.rewriting.basic.FromFunctionNodeRewriter"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inverse_to_diag_reciprocal_dumb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c2be2",
   "metadata": {},
   "source": [
    "Before the rewrite, the graph is quite simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80040e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixInverse [id A] 0\n",
      " └─ x [id B]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytensor.graph.rewriting import rewrite_graph\n",
    "from pytensor.graph.fg import FunctionGraph\n",
    "\n",
    "x_inv = MatrixInverse()(x)\n",
    "fg_inv = FunctionGraph([x], [x_inv])\n",
    "fg_inv.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a17e1",
   "metadata": {},
   "source": [
    "After the rewrite, the graph seems to become more complex! But notice that the outer graph is just `AllocDiag` (i.e. putting values on the diagonal of a matrix) of `1 / diag(x)`, so we've made things simplier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6b60eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AllocDiag{self.axis1=0, self.axis2=1, self.offset=0} [id A] 3\n",
      " └─ True_div [id B] 2\n",
      "    ├─ ExpandDims{axis=0} [id C] 1\n",
      "    │  └─ 1 [id D]\n",
      "    └─ ExtractDiag{offset=0, axis1=0, axis2=1, view=False} [id E] 0\n",
      "       └─ x [id F]\n",
      "\n",
      "Inner graphs:\n",
      "\n",
      "AllocDiag{self.axis1=0, self.axis2=1, self.offset=0} [id A]\n",
      " ← AdvancedSetSubtensor [id G]\n",
      "    ├─ Alloc [id H]\n",
      "    │  ├─ 0.0 [id I]\n",
      "    │  ├─ Add [id J]\n",
      "    │  │  ├─ Subtensor{i} [id K]\n",
      "    │  │  │  ├─ Shape [id L]\n",
      "    │  │  │  │  └─ *0-<Vector(float64, shape=(?,))> [id M]\n",
      "    │  │  │  └─ -1 [id N]\n",
      "    │  │  └─ 0 [id O]\n",
      "    │  └─ Add [id J]\n",
      "    │     └─ ···\n",
      "    ├─ *0-<Vector(float64, shape=(?,))> [id M]\n",
      "    ├─ Add [id P]\n",
      "    │  ├─ ARange{dtype='int64'} [id Q]\n",
      "    │  │  ├─ 0 [id R]\n",
      "    │  │  ├─ Subtensor{i} [id S]\n",
      "    │  │  │  ├─ Shape [id T]\n",
      "    │  │  │  │  └─ *0-<Vector(float64, shape=(?,))> [id M]\n",
      "    │  │  │  └─ -1 [id U]\n",
      "    │  │  └─ 1 [id V]\n",
      "    │  └─ ExpandDims{axis=0} [id W]\n",
      "    │     └─ 0 [id X]\n",
      "    └─ Add [id Y]\n",
      "       ├─ ARange{dtype='int64'} [id Q]\n",
      "       │  └─ ···\n",
      "       └─ ExpandDims{axis=0} [id Z]\n",
      "          └─ 0 [id BA]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumb_rewrite = in2out(inverse_to_diag_reciprocal_dumb, name=\"inverse_to_diag_reciprocal\")\n",
    "dumb_rewrite.rewrite(fg_inv)\n",
    "fg_inv.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88330eb5",
   "metadata": {},
   "source": [
    "We already achieve the desired speed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d7f2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dumb = pytensor.function(fg_inv.inputs, fg_inv.outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b937f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268 μs ± 5.75 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit f_dumb(x_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecdbe2",
   "metadata": {},
   "source": [
    "And we're doing the rigth thing for diagonal inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3bc8bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(f_dumb(x_diag), np.linalg.inv(x_diag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70191de0",
   "metadata": {},
   "source": [
    "...but the answer is wrong if the input is not diagonal. That's why this is a \"dumb\" implementation.\n",
    "\n",
    "Also this is a very important point about pytensor rewrites! They just do what you tell them -- there is *no* input/output validation, except what you do yourself! We need to be sure that:\n",
    "\n",
    "1. The inputs we're getting meet our expectations (in this case, that they are diagonal)\n",
    "2. The outputs we're returing are doing the right thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc0e4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------Correct Answer-------------------------------------------\n",
      "[[ 0.243 -0.322  0.273  0.107  0.219  0.145 -0.304 -0.325  0.063 -0.484]\n",
      " [ 1.762 -0.271 -1.126  2.86   0.463 -0.537  2.033 -0.671  0.848  3.911]\n",
      " [ 0.615 -0.239 -0.51   1.281  0.524  0.367  1.043  0.03   0.798  1.83 ]\n",
      " [ 0.574 -0.666 -0.579  1.969  0.266  0.528  1.186 -0.269  1.306  1.878]\n",
      " [-0.408 -0.171  1.121 -0.826 -0.036  0.628 -0.873 -0.184 -0.147 -1.687]\n",
      " [ 0.251 -1.005  1.309  0.673  0.021  0.762  0.215 -0.65   0.032 -0.09 ]\n",
      " [-0.487 -0.147  0.658 -1.098 -0.067  0.375 -0.584 -0.048 -0.137 -1.452]\n",
      " [-0.425  0.482  0.084 -1.41  -0.079 -0.133 -0.968  0.852  0.001 -1.401]\n",
      " [-0.122 -0.09  -0.197  0.128 -0.022 -0.141  0.157 -0.139  0.099  0.542]\n",
      " [-0.569  0.449  0.329 -1.787 -0.257  0.194 -1.447  0.679 -0.867 -1.942]]\n",
      "---------------------------------------------Our Answer---------------------------------------------\n",
      "[[-7.86   0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     2.055  0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     3.635  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.    12.925  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.    -4.044  0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.    -2.004  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.651  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.041  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     1.997  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     1.72 ]]\n"
     ]
    }
   ],
   "source": [
    "x_dense = rng.normal(size=(10, 10))\n",
    "with np.printoptions(linewidth=1000, precision=3, suppress=True):\n",
    "    print('Correct Answer'.center(100, '-'))\n",
    "    print(np.linalg.inv(x_dense))\n",
    "    print('Our Answer'.center(100, '-'))\n",
    "    print(f_dumb(x_dense))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a89df5",
   "metadata": {},
   "source": [
    "## Improving the rewrite\n",
    "\n",
    "So we've seen some problems with this approach. We are just blindly applying `diag(1/diag(x))` , without thinking about whether such an operation is even valid. Also, you will notice that I imported and used `MatrixInverse`, rather than just using `pt.linalg.inv` directly. What's up with that? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ec579",
   "metadata": {},
   "source": [
    "### Sidebar: `Blockwise`\n",
    "\n",
    "Let's tackle these in reverse order. To understand why we had to import and use `MatrixInverse`, let's look carefully at the graph we get from `pt.linalg.inv`.\n",
    "\n",
    "Notice that the `Op` is not precisely `MatrixInverse`. Instead, it's `Blockwise(MatrixInverse)`. `Blockwise` is pytensor's method of vectorizing non-scalar functions. For the boomers who took the old SAT, `blockwise:array::elemwise:scalar`\n",
    "\n",
    "This means that our rewrite won't see the `MatrixInverse` in this graph, because there isn't one! Instead, there's a `Blockwise`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36e652d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockwise{MatrixInverse, (m,m)->(m,m)} [id A]\n",
      " └─ x [id B]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_inv_2 = pt.linalg.inv(x)\n",
    "x_inv_2.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d6127",
   "metadata": {},
   "source": [
    "Here's an example of the `Blockwise` in action, inverting a stack of 5 $3 \\times 3$ matrices. We're going to have to reason about this in the inputs and the outputs of our rewrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb8905f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6.92194802e-01,  5.09370231e-01,  4.64154574e-01],\n",
       "        [-1.46435306e-01,  1.66640236e-01, -3.36582961e-01],\n",
       "        [-1.24480589e-01,  1.07943060e+00,  3.10903260e+00]],\n",
       "\n",
       "       [[-1.30376220e+00,  1.30799229e+00,  7.40815577e-01],\n",
       "        [ 2.24919563e-05,  1.93682707e-01,  7.81099186e-01],\n",
       "        [ 4.22971416e-01, -1.44250754e+00, -6.25374158e-01]],\n",
       "\n",
       "       [[ 5.00204994e+00,  1.77425105e+00,  5.36564833e+00],\n",
       "        [ 1.21396036e+00,  2.66541882e-01,  5.49489465e-01],\n",
       "        [-1.43708864e+00, -1.24884332e+00, -1.34443727e+00]],\n",
       "\n",
       "       [[-7.25963387e-02, -6.88130588e-02, -7.67758384e-01],\n",
       "        [ 2.08946446e-01, -7.37595836e-01, -1.22756192e-01],\n",
       "        [-8.80446815e-01, -2.36095937e-02,  5.60986806e-01]],\n",
       "\n",
       "       [[ 1.86795703e-01, -3.39653175e-01, -8.04111982e-02],\n",
       "        [-2.78581768e-01, -2.27589150e-01, -3.98090359e-01],\n",
       "        [ 9.07430228e-02,  4.47713122e-01, -1.88976517e-01]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batched = pt.tensor('x_batched', shape=(None, None, None))\n",
    "f_inv_batched = pytensor.function([x_batched], pt.linalg.inv(x_batched))\n",
    "\n",
    "x_batched_val = rng.normal(size=(5, 3, 3))\n",
    "f_inv_batched(x_batched_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047e750",
   "metadata": {},
   "source": [
    "For the inputs, we need to have our rewrite track `Blockwise` instead of `MatrixInverse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "391ca902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytensor.tensor.blockwise import Blockwise\n",
    "\n",
    "@node_rewriter([Blockwise])\n",
    "def inverse_to_diag_reciprocal_blockwise_wrong(fgraph, node):\n",
    "    [x] = node.inputs\n",
    "    return [pt.diag(1 / pt.diag(x))]\n",
    "\n",
    "blockwise_rewrite_wrong = in2out(inverse_to_diag_reciprocal_blockwise_wrong, \n",
    "                                 name=\"inverse_to_diag_reciprocal_blockwise_wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c064a",
   "metadata": {},
   "source": [
    "This worked, but now it's catastrophically wrong, because it will replace *any* Blockwise operation with `diag(1/diag(x))`! For example, `Dot` is also blockwise. In this case, the rewrite will error, because we assumed there should only be one input. But `Dot` has two inputs! \n",
    "\n",
    "You might have seen cryptic rewrite errors like this in PyMC models. It's telling you that you found a case where the input assumptions of some rewrite or another are failing. You should open an issue if you hit this! But notice that it will gracefully fail and carry on. So this will never make your graphs *wrong* (assuming it wasn't already wrong), but if you see this, you might be missing out on optimizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "262b37f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: inverse_to_diag_reciprocal_blockwise_wrong\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Blockwise{dot, (m,k),(k,n)->(m,n)}(x, x.T)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/home/jesse/mambaforge/envs/econ/lib/python3.12/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jesse/mambaforge/envs/econ/lib/python3.12/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_275155/3360196437.py\", line 5, in inverse_to_diag_reciprocal_blockwise_wrong\n",
      "    [x] = node.inputs\n",
      "    ^^^\n",
      "ValueError: too many values to unpack (expected 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockwise{dot, (m,k),(k,n)->(m,n)} [id A] 1\n",
      " ├─ x [id B]\n",
      " └─ Transpose{axes=[1, 0]} [id C] 'x.T' 0\n",
      "    └─ x [id B]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_inner = x @ x.T\n",
    "fg_inner = FunctionGraph([x], [x_inner])\n",
    "blockwise_rewrite_wrong.rewrite(fg_inner)\n",
    "fg_inner.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660e311",
   "metadata": {},
   "source": [
    "So this is a mess. What we need to do is introduce some logic to make sure we only hit `MatrixInverse`. Blockwise has a property called `core_op`, which stores the `Op` that it vectorizes. We need to check that the `core_op` is `MatrixInverse`, and only rewrite those cases.\n",
    "\n",
    "Important fact -- if a rewriter returns `None` or `False`, it will not apply rewrites. So we need to return `None` if `not isinstance(node.op.core_op, MatrixInverse)`, otherwise do the rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5792a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@node_rewriter([Blockwise])\n",
    "def inverse_to_diag_reciprocal_blockwise_less_wrong(fgraph, node):\n",
    "    core_op = node.op.core_op\n",
    "    if not isinstance(core_op, MatrixInverse):\n",
    "        return\n",
    "    \n",
    "    [x] = node.inputs\n",
    "    return [pt.diag(1 / pt.diag(x))]\n",
    "\n",
    "blockwise_rewrite_less_wrong = in2out(inverse_to_diag_reciprocal_blockwise_less_wrong,\n",
    "                                      name=\"inverse_to_diag_reciprocal_blockwise_less_wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278670c",
   "metadata": {},
   "source": [
    "The rewrite now applies to this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf4702a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AllocDiag{self.axis1=0, self.axis2=1, self.offset=0} [id A] 3\n",
      " └─ True_div [id B] 2\n",
      "    ├─ ExpandDims{axis=0} [id C] 1\n",
      "    │  └─ 1 [id D]\n",
      "    └─ ExtractDiag{offset=0, axis1=0, axis2=1, view=False} [id E] 0\n",
      "       └─ x [id F]\n",
      "\n",
      "Inner graphs:\n",
      "\n",
      "AllocDiag{self.axis1=0, self.axis2=1, self.offset=0} [id A]\n",
      " ← AdvancedSetSubtensor [id G]\n",
      "    ├─ Alloc [id H]\n",
      "    │  ├─ 0.0 [id I]\n",
      "    │  ├─ Add [id J]\n",
      "    │  │  ├─ Subtensor{i} [id K]\n",
      "    │  │  │  ├─ Shape [id L]\n",
      "    │  │  │  │  └─ *0-<Vector(float64, shape=(?,))> [id M]\n",
      "    │  │  │  └─ -1 [id N]\n",
      "    │  │  └─ 0 [id O]\n",
      "    │  └─ Add [id J]\n",
      "    │     └─ ···\n",
      "    ├─ *0-<Vector(float64, shape=(?,))> [id M]\n",
      "    ├─ Add [id P]\n",
      "    │  ├─ ARange{dtype='int64'} [id Q]\n",
      "    │  │  ├─ 0 [id R]\n",
      "    │  │  ├─ Subtensor{i} [id S]\n",
      "    │  │  │  ├─ Shape [id T]\n",
      "    │  │  │  │  └─ *0-<Vector(float64, shape=(?,))> [id M]\n",
      "    │  │  │  └─ -1 [id U]\n",
      "    │  │  └─ 1 [id V]\n",
      "    │  └─ ExpandDims{axis=0} [id W]\n",
      "    │     └─ 0 [id X]\n",
      "    └─ Add [id Y]\n",
      "       ├─ ARange{dtype='int64'} [id Q]\n",
      "       │  └─ ···\n",
      "       └─ ExpandDims{axis=0} [id Z]\n",
      "          └─ 0 [id BA]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg_blockwise = FunctionGraph([x], [pt.linalg.inv(x)])\n",
    "blockwise_rewrite_less_wrong.rewrite(fg_blockwise);\n",
    "fg_blockwise.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97150c8a",
   "metadata": {},
   "source": [
    "And does *not* error on the `dot` graph, because it didn't find a `MatrixInverse` in that graph, even though it did find a `Blockwise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c9093f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockwise{dot, (m,k),(k,n)->(m,n)} [id A] 1\n",
      " ├─ x [id B]\n",
      " └─ Transpose{axes=[1, 0]} [id C] 'x.T' 0\n",
      "    └─ x [id B]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blockwise_rewrite_less_wrong.rewrite(fg_inner)\n",
    "fg_inner.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c94fc5",
   "metadata": {},
   "source": [
    "But we still haven't reasoned about the outputs! If we try to pass in a batched input, we will get an error, because `pt.diag` is only defined for `1d` and `2d` inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aec41dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: inverse_to_diag_reciprocal_blockwise_less_wrong\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Blockwise{MatrixInverse, (m,m)->(m,m)}(x_batched)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/home/jesse/mambaforge/envs/econ/lib/python3.12/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jesse/mambaforge/envs/econ/lib/python3.12/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_275155/703198838.py\", line 8, in inverse_to_diag_reciprocal_blockwise_less_wrong\n",
      "    return [pt.diag(1 / pt.diag(x))]\n",
      "                        ^^^^^^^^^^\n",
      "  File \"/home/jesse/mambaforge/envs/econ/lib/python3.12/site-packages/pytensor/tensor/basic.py\", line 3994, in diag\n",
      "    raise ValueError(\"Input must be 1- or 2-d.\")\n",
      "ValueError: Input must be 1- or 2-d.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fg_batched = FunctionGraph([x_batched], [pt.linalg.inv(x_batched)])\n",
    "blockwise_rewrite_less_wrong.rewrite(fg_batched)\n",
    "f_inv_batched_rewrite = pytensor.function(fg_batched.inputs, fg_batched.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89d50b1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockwise{MatrixInverse, (m,m)->(m,m)} [id A] 0\n",
      " └─ x_batched [id B]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_inv_batched_rewrite.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d16255",
   "metadata": {},
   "source": [
    "To fix this, we can return a vectorized graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08daad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytensor.graph.replace import vectorize_graph\n",
    "\n",
    "@node_rewriter([Blockwise])\n",
    "def inverse_to_diag_reciprocal_blockwise_least_wrong(fgraph, node):\n",
    "    core_op = node.op.core_op\n",
    "    if not isinstance(core_op, MatrixInverse):\n",
    "        return\n",
    "    \n",
    "    [x] = node.inputs\n",
    "    x_core = pt.matrix('x', dtype=x.dtype)\n",
    "    x_inv = pt.diag(1 / pt.diag(x_core))\n",
    "    \n",
    "    return [vectorize_graph(x_inv, {x_core: x})]\n",
    "\n",
    "blockwise_rewrite_least_wrong = in2out(inverse_to_diag_reciprocal_blockwise_least_wrong,\n",
    "                                       name=\"inverse_to_diag_reciprocal_blockwise_least_wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03524d1",
   "metadata": {},
   "source": [
    "Now we get a blockwise, and return a blockwise! And there's no more errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f10fab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockwise{AllocDiag{self.axis1=0, self.axis2=1, self.offset=0}, (i00)->(o00,o01)} [id A] 4\n",
      " └─ True_div [id B] 3\n",
      "    ├─ ExpandDims{axis=0} [id C] 2\n",
      "    │  └─ ExpandDims{axis=0} [id D] 1\n",
      "    │     └─ 1 [id E]\n",
      "    └─ ExtractDiag{offset=0, axis1=1, axis2=2, view=False} [id F] 0\n",
      "       └─ x_batched [id G]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blockwise_rewrite_least_wrong.rewrite(fg_batched)\n",
    "fg_batched.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c26f6",
   "metadata": {},
   "source": [
    "Of course, numerically, the answer is still wrong :D\n",
    "\n",
    "That brings us to the next issue, which is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05c96873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.90719681,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.36184177,  0.        ],\n",
       "        [ 0.        ,  0.        ,  4.20986532]],\n",
       "\n",
       "       [[-0.93439403,  0.        ,  0.        ],\n",
       "        [ 0.        , -1.87181569,  0.        ],\n",
       "        [ 0.        ,  0.        ,  3.72068559]],\n",
       "\n",
       "       [[-8.98064729,  0.        ,  0.        ],\n",
       "        [ 0.        , -2.98645156,  0.        ],\n",
       "        [ 0.        ,  0.        ,  3.58822671]],\n",
       "\n",
       "       [[-1.27978025,  0.        ,  0.        ],\n",
       "        [ 0.        , -0.7440512 ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  7.85069584]],\n",
       "\n",
       "       [[ 0.36090306,  0.        ,  0.        ],\n",
       "        [ 0.        , -2.85130997,  0.        ],\n",
       "        [ 0.        ,  0.        , -0.58224815]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_inv_batched_rewrite = pytensor.function(fg_batched.inputs, fg_batched.outputs[0])\n",
    "f_inv_batched_rewrite(x_batched_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b995246",
   "metadata": {},
   "source": [
    "### Detecting a diagonal matrix\n",
    "\n",
    "The next problem with our rewrite is that it is converting *every* inverse to the reciprocol of the diagonal, which is not correct. `x_batched_val` input above is *not* a diagonal matrix. So how can we address this?\n",
    "\n",
    "There are several design choices that we could make this this point:\n",
    "\n",
    "\n",
    "#### Tags\n",
    "We could \"tag\" matrices. In pytensor, all variables can take a \"tags\" keyword, which we can use to store metadata about the matrix. For example, we could have users tag a matrix as \"diagonal\", then look for that tag in our rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84922fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_diag_tag = pt.tensor('x', shape=(None, None))\n",
    "x_diag_tag.tag.diagonal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13dcf0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(x_diag_tag.tag, 'diagonal', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "968f36f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(x.tag, 'diagonal', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4deb99",
   "metadata": {},
   "source": [
    "This might seem like a nice solution. Some older rewrites still use it! But it's actually pretty bad. For example, if we do `2 * x_diag_tag`, we lose the tag, even though we know that $a \\otimes X$ is diagonal if $X$ is diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32d131f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * x_diag_tag\n",
    "getattr(y.tag, 'diagonal', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529bd4e3",
   "metadata": {},
   "source": [
    "Also, it's a really hidden feature. Users can't really give us this information to help apply rewrites or not. Did you know you can store arbitrary information in `tensor.tag`? Exactly.\n",
    "\n",
    "So I would argue that this is not a real solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe1845",
   "metadata": {},
   "source": [
    "#### TypeOp\n",
    "\n",
    "A better solution to tagging would be to introduce a `TypeOp`, which would work like the `ShapeOp`. It would be a graph-to-graph transformation that could reason about the algebraic types of nodes in a graph. \n",
    "\n",
    "This is a possibility I am very excited about, because it would have a lot of different applications. It would ease rewrites like these, but could also:\n",
    "\n",
    "1. Detect if a graph output is convex, to allow us to dispatch to convex solvers like e.g. CLARABEL\n",
    "2. Detect if variables are positive/negative/real, and avoid illegal rewrites\n",
    "3. Allow users to help us do good rewrites by giving information about a wide range of types, including diagonal, banded, topelitz, orthonormal, etc, etc. \n",
    "\n",
    "Actually doing this is beyond the scope of this talk. But if you're interested in this, contact me so we can collaborate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcc02e",
   "metadata": {},
   "source": [
    "#### Reason about the inputs\n",
    "\n",
    "Since we don't have a `TypeOp` to check, and `tags` are very fragile, we're left to reason about the inputs. \n",
    "\n",
    "What I mean is, we need to think about how a diagonal matrix can come about. It could happen if:\n",
    "\n",
    "1. The user passes data that is diagonal in the first place\n",
    "2. We have a vector that is passed into `AllocDiag`\n",
    "3. We have anything multiplied with an identity matrix\n",
    "\n",
    "Case (1) is hopeless unless we have constant data. Let's leave that case aside and focus on the other two. We need to:\n",
    "\n",
    "- Pull out the input to the `MatrixInverse`\n",
    "- Check if its owner op is an `AllocDiag`\n",
    "- If so, we proceed\n",
    "- If not, we check if its owner op is an `Elemwise(multiplication)`.\n",
    "- If so, we check if there are exactly 2 inputs to the owner op, and that one of them is `Eye`\n",
    "- If so, we proceed using the diagonal of the non-Eye input \n",
    "- If not, we bail on the rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb0fbb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytensor.tensor.elemwise import Elemwise\n",
    "from pytensor.tensor.basic import Eye, AllocDiag\n",
    "from pytensor.scalar.basic import Mul\n",
    "\n",
    "@node_rewriter([Blockwise])\n",
    "def inverse_to_diag_reciprocal_blockwise_correct(fgraph, node):\n",
    "    core_op = node.op.core_op\n",
    "    if not isinstance(core_op, MatrixInverse):\n",
    "        return\n",
    "    \n",
    "    [x] = node.inputs\n",
    "    \n",
    "    # If x is a root, we can't do anything with it (no info!)\n",
    "    if not x.owner:\n",
    "        return\n",
    "    \n",
    "    if (isinstance(x.owner.op, Elemwise) and \n",
    "        isinstance(x.owner.op.scalar_op, Mul)):\n",
    "        \n",
    "        mul_inputs = x.owner.inputs\n",
    "        if len(mul_inputs) > 2:\n",
    "            return\n",
    "        if not any(mul_input.owner and isinstance(mul_input.owner.op, Eye) for mul_input in mul_inputs):\n",
    "            return\n",
    "    \n",
    "    elif not isinstance(x.owner.op, AllocDiag):\n",
    "        return\n",
    "    \n",
    "    # If we got here, x is a diagonal matrix, so we can proceed\n",
    "    \n",
    "    x_core = pt.matrix('x', dtype=x.dtype)\n",
    "    x_inv = pt.diag(1 / pt.diag(x_core))\n",
    "    \n",
    "    return [vectorize_graph(x_inv, {x_core: x})]\n",
    "\n",
    "rewrite_correct = in2out(inverse_to_diag_reciprocal_blockwise_correct, \n",
    "                         name='inverse_to_diag_reciprocal_blockwise_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a37a8bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scalar = pt.dscalar('x_scalar')\n",
    "x_vec = pt.dvector('x_vec')\n",
    "x_dense = pt.dmatrix('x_dense')\n",
    "\n",
    "fg_should_rewrite = FunctionGraph([x_vec], [pt.linalg.inv(pt.diag(x_vec))])\n",
    "fg_should_rewrite_2 = FunctionGraph([x_scalar], [pt.linalg.inv(pt.eye(10) * x_scalar)])\n",
    "fg_should_not_rewrite = FunctionGraph([x_dense], [pt.linalg.inv(x_dense)])\n",
    "\n",
    "rewrite_correct.rewrite(fg_should_rewrite);\n",
    "rewrite_correct.rewrite(fg_should_rewrite_2);\n",
    "rewrite_correct.rewrite(fg_should_not_rewrite);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "740dcc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockwise{AllocDiag{self.axis1=0, self.axis2=1, self.offset=0}, (i00)->(o00,o01)} [id A] 4\n",
      " └─ True_div [id B] 3\n",
      "    ├─ ExpandDims{axis=0} [id C] 2\n",
      "    │  └─ 1 [id D]\n",
      "    └─ ExtractDiag{offset=0, axis1=0, axis2=1, view=False} [id E] 1\n",
      "       └─ AllocDiag{self.axis1=0, self.axis2=1, self.offset=0} [id F] 0\n",
      "          └─ x_vec [id G]\n",
      "\n",
      "Inner graphs:\n",
      "\n",
      "AllocDiag{self.axis1=0, self.axis2=1, self.offset=0} [id F]\n",
      " ← AdvancedSetSubtensor [id H]\n",
      "    ├─ Alloc [id I]\n",
      "    │  ├─ 0.0 [id J]\n",
      "    │  ├─ Add [id K]\n",
      "    │  │  ├─ Subtensor{i} [id L]\n",
      "    │  │  │  ├─ Shape [id M]\n",
      "    │  │  │  │  └─ *0-<Vector(float64, shape=(?,))> [id N]\n",
      "    │  │  │  └─ -1 [id O]\n",
      "    │  │  └─ 0 [id P]\n",
      "    │  └─ Add [id K]\n",
      "    │     └─ ···\n",
      "    ├─ *0-<Vector(float64, shape=(?,))> [id N]\n",
      "    ├─ Add [id Q]\n",
      "    │  ├─ ARange{dtype='int64'} [id R]\n",
      "    │  │  ├─ 0 [id S]\n",
      "    │  │  ├─ Subtensor{i} [id T]\n",
      "    │  │  │  ├─ Shape [id U]\n",
      "    │  │  │  │  └─ *0-<Vector(float64, shape=(?,))> [id N]\n",
      "    │  │  │  └─ -1 [id V]\n",
      "    │  │  └─ 1 [id W]\n",
      "    │  └─ ExpandDims{axis=0} [id X]\n",
      "    │     └─ 0 [id Y]\n",
      "    └─ Add [id Z]\n",
      "       ├─ ARange{dtype='int64'} [id R]\n",
      "       │  └─ ···\n",
      "       └─ ExpandDims{axis=0} [id BA]\n",
      "          └─ 0 [id BB]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg_should_rewrite.dprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3fb5c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpecifyShape [id A] 7\n",
      " ├─ Blockwise{AllocDiag{self.axis1=0, self.axis2=1, self.offset=0}, (i00)->(o00,o01)} [id B] 6\n",
      " │  └─ True_div [id C] 5\n",
      " │     ├─ ExpandDims{axis=0} [id D] 4\n",
      " │     │  └─ 1 [id E]\n",
      " │     └─ ExtractDiag{offset=0, axis1=0, axis2=1, view=False} [id F] 3\n",
      " │        └─ Mul [id G] 2\n",
      " │           ├─ Eye{dtype='float64'} [id H] 1\n",
      " │           │  ├─ 10 [id I]\n",
      " │           │  ├─ 10 [id J]\n",
      " │           │  └─ 0 [id K]\n",
      " │           └─ ExpandDims{axes=[0, 1]} [id L] 0\n",
      " │              └─ x_scalar [id M]\n",
      " ├─ 10 [id N]\n",
      " └─ 10 [id O]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg_should_rewrite_2.dprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71187775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockwise{MatrixInverse, (m,m)->(m,m)} [id A] 0\n",
      " └─ x_dense [id B]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f2a1cd2edd0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg_should_not_rewrite.dprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51f7ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_should_1 = pytensor.function(fg_should_rewrite.inputs, fg_should_rewrite.outputs)\n",
    "f_should_2 = pytensor.function(fg_should_rewrite_2.inputs, fg_should_rewrite_2.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0132dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  1.6,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ],\n",
      "       [  0. , -22.6,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ],\n",
      "       [  0. ,   0. ,  -1.3,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ],\n",
      "       [  0. ,   0. ,   0. ,   2.1,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ],\n",
      "       [  0. ,   0. ,   0. ,   0. ,   2.3,   0. ,   0. ,   0. ,   0. ,   0. ],\n",
      "       [  0. ,   0. ,   0. ,   0. ,   0. ,  -2.1,   0. ,   0. ,   0. ,   0. ],\n",
      "       [  0. ,   0. ,   0. ,   0. ,   0. ,   0. ,  -7. ,   0. ,   0. ,   0. ],\n",
      "       [  0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. , -10.4,   0. ,   0. ],\n",
      "       [  0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   1.6,   0. ],\n",
      "       [  0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,  -7.6]])]\n",
      "[array([[0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "       [0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "       [0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "       [0. , 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "       [0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. ],\n",
      "       [0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. , 0. ],\n",
      "       [0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ],\n",
      "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. ],\n",
      "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. ],\n",
      "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.5]])]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(linewidth=1000, precision=1, suppress=True):\n",
    "    print(f_should_1(rng.normal(size=(10))))\n",
    "    print(f_should_2(rng.normal(size=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23ff70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
